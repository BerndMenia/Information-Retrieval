### Exercise 2 (0 / 3 points)
Please read the following paper by Norbert Fuhr, in which he raises a few important points on MRRand further metrics in the information retrieval field. This should give you a critical perspective on evaluation setups.Norbert Fuhr. Some Common Mistakes In IR Evaluation, And How They Can Be Avoided.SIGIR Forum, 51(3):32–41, 2018.  ISSN 0163-5840.  doi:10.1145/3190580.3190586.  URL http://sigir.org/wp-content/uploads/2018/01/p032.pdfAnswer the following questions:
  
- a (1 Point): Why is MRR (or ERR) not a good choice for assessing ranked lists, according to Fuhr?

- b (1 Point): What is the problem with MAP?

- c (1 Point): Why is it not advisable to use simple holdout evaluation?

.....................................................................................................

- a) Mean Reciprocal Rank (MRR) has unwanted behaviour on some queries. Fuhr makes the following example: Suppose you have two Systems A and B. On three given queries A returns the relevant docmunts at ranks 1, 2 and 4 respectively. On the other hand B returns rank 2 for all three queries. Regarding A we retrieve the first relevant document at rank 2.33 = (1 + 2 + 4) / 3. For B the same calculation results in 2 = (2 + 2 + 2) / 3. In other words B has a better mean. However when we calculate the actual MRR then the results change: For A we now calculate 0.58 = (1/1 + 1/2 + 1/4) / 3 which is better than the result for B, which is 0.5. So even though B outperformed A on average ranks, A was better when we applied MRR to it, which is unintuitive. 

  This is not all though. There is another problem with using MRR. Due to how MRR work the difference between ranks 1 and 2 is the same as between 2 and infinity (in theory). In other words MRR is an ordinal scale, not an interval scale. However you cannot compute the mean of an ordinal scale because of an infinite amount of values. You could use the median instead of the mean, but this would result in mostly ties. Some authors proposed ERR, which is a generalization of the reciprocal rank and also takes a probability to stop into account: sum_from_r=1_to_n(1/r)*P (user stops at position r). However ERR also does not solve the problem that MRR has. Fuhr proposes an adapted vversion of the just sen formula: sum_from_r=1_to_n(r)*P (user stops at position r). 

- b) The problem of Mean Average Precision (MAP) is a simple one: For the most part users cannot be bothered to look past the first relevant retrieved document and the results are therefore most likely skewed. According to Dogan et al. the distribution follows the power law. A proposed solution to this problem is Rank-Biased Preicion (RBP) which includes a parameter p that describes the probability of a user to go to the next rank. The formula of users stopping at rank k is s(k) = (1-p)*p^(k-1). But RBP comes with its own problems and is therefore not a perfect solution. 

- c) The (standard) holdout method is to split data into training and test data. Most often the training data makes up 80% of the data. However studies have shown that the performance varies upt to 20% depending on the split. This variation can be easily overcome by using cross-validation, so that's not a big deal. 

  Sometimes a third set, the tuning set, is used to determine the best parameters. "For k-fold cross-validation, after putting aside one fold after the other for testing, we can either use just one fold for tuning and the remaining k − 2 sets for training, or we can perform an inner loop where we rotate the tuning fold over the k − 1 folds. In evaluation initiatives, the setting usually corresponds to the simple holdout method, where participants are initially given some training data, and get access to the testing set only for performing their runs, while the ground truth (e.g., relevance judgments) for this set is disclosed later." Better results than the simple holdout method can be achieved though. 